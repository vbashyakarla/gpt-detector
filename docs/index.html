<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT Detection Bias</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
    h1 {
        color: #2f3e46;
        border-bottom: 2px solid #588157;
        padding-bottom: 10px;
    }
    .link-button {
        display: inline-block;
        padding: 10px 20px;
        background-color: #588157;
        color: white;
        text-decoration: none;
        border-radius: 4px;
        margin-top: 20px;
    }
    .link-button:hover {
        background-color: #3a5a40;
    }    </style>
</head>
<body>
    <div class="container">
        <h1>Bias in GPT Detection Against Non-Native English Speakers </h1>
        
        <h2>Project Overview</h2>
        <p>The exploration independently confirms the finding of a 2023 CellPress Patterns <a 
        href="https://linkinghub.elsevier.com/retrieve/pii/S2666389923001307"> publication</a>
        by a team of Stanford researchers that detectors used to distinguish whether text was 
        AI-generated or human-generated perform better 
        on texts written by native English speakers than on texts by non-native English speakers. 
        Text written by non-native speakers are much more likely to be deemed AI-generated 
        by the detectors evaluated.
        <br>
        <br>
        This finding raises ethical questions, as the application of these detectors risks harming contributions of non-native English speakers, 
        particularly when such detectors are used in evaluative settings.
       <br>
        <br>
        The accompanying anlaysis extends the findings furnished in the paper and its supplements with a 
        detailed view at the performance of individual detectors and their performance on texts written 
        by native and non-native English speakers. It also contextualizes these findings relative to 
        detector performance on AI-generated texts.
       <br>
        <br>

        </p>

        <h2>Related Links</h2>
        <ul>
            <li><a href="https://github.com/vbashyakarla/gpt-detector">GitHub Repo</a></li>
            <li><a href="https://github.com/vbashyakarla/gpt-detector/tree/main/data">Source Data</a></li>

        </ul>
        <a href="https://github.com/vbashyakarla/gpt-detector/blob/main/git-detector-bias.ipynb" class="link-button">View Analysis</a>
    </div>
</body>
</html>
